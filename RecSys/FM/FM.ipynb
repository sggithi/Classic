{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Colab setting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DSAIL')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/DSAIL')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp2VLPKnmids",
        "outputId": "a1c6f882-b8e7-4b8a-b031-b29e76723312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfEsegib1jW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee551187-3287-46a0-e597-da0d21e9d25c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Data U1.base로 train, u1.test로 test data만들기\n",
        "# U.data는 현재 ([\"user_id\", \"movie_id\", \"rating\", \"timestamp\"])로 구성\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "file_path = \"ml-100k/u1.base\"\n",
        "columns = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        "train_data = pd.read_csv(file_path, sep='\\t', names=columns)\n",
        "\n",
        "\n",
        "user_movie_matrix = train_data.pivot(index='user_id', columns='movie_id', values='rating')\n",
        "train = user_movie_matrix.fillna(0)\n",
        "\n",
        "file_path = \"ml-100k/u1.test\"\n",
        "columns = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        "test_data = pd.read_csv(file_path, sep='\\t', names=columns)\n",
        "user_movie_matrix = test_data.pivot(index='user_id', columns='movie_id', values='rating')\n",
        "test = user_movie_matrix.fillna(0)\n",
        "\n",
        "# GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#input data가 {user, rated movie, other movie's rating, Time, Last Movie rated| one-hot- encoding으로\n",
        "# y는 score\n",
        "\n",
        "num_user_train  = train_data[\"user_id\"].max()\n",
        "num_movie_train = train_data[\"movie_id\"].max()\n",
        "num_user_test  = test_data[\"user_id\"].max()\n",
        "num_movie_test = test_data[\"movie_id\"].max()\n",
        "\n",
        "num_features_train = len(train_data)\n",
        "num_features_test = len(test_data)\n",
        "\n",
        "# Ratings\n",
        "test_R = np.load(\"continous_test.npy\")\n",
        "train_R = np.load(\"continous_train.npy\")\n",
        "\n",
        "# test_R 1591 -> 1682\n",
        "num = 1682 - 1591\n",
        "test_R = np.concatenate([test_R, np.zeros((test_R.shape[0], num))], axis=1)\n",
        "\n",
        "# normalized_ratings\n",
        "normalized_ratings_test = test_R / (test_R.sum(axis=1, keepdims=True) + 1e-13)\n",
        "normalized_ratings_train = train_R / (train_R.sum(axis = 1, keepdims=True) +1e-13)\n",
        "\n",
        "# user & movie one-hot encoding\n",
        "# user_id_onehot = pd.get_dummies(train_data['user_id'], prefix='user')\n",
        "# movie_id_onehot = pd.get_dummies(train_data['movie_id'], prefix='movie')   이러면 movie id가 연속이 아니라서 1683개의 movie가 있는데 1650 column만 생김\n",
        "\n",
        "user_train = np.zeros((num_features_train, num_user_train))\n",
        "movie_train = np.zeros((num_features_train, num_movie_train))\n",
        "other_ratings_train = np.zeros((num_features_train, num_movie_train))\n",
        "time_train = (train_data[\"timestamp\"]-874724727) / 18561911\n",
        "last_movie_train = np.zeros((num_features_train, num_movie_train))\n",
        "\n",
        "\n",
        "for i in range(num_features_train ):\n",
        "  user_train[i, train_data.iloc[i]['user_id'] - 1] = 1\n",
        "  movie_train[i, train_data.iloc[i]['movie_id'] - 1] = 1\n",
        "  other_ratings_train[i, :] = normalized_ratings_train[train_data.iloc[i]['user_id'] - 1, :]\n",
        "  if i > 0:\n",
        "    if train_data.iloc[i - 1]['user_id'] == train_data.iloc[i]['user_id']:\n",
        "      last_movie_train[i, train_data.iloc[i - 1]['movie_id'] - 1] = 1\n",
        "\n",
        "user_test = np.zeros((num_features_test, num_user_train))\n",
        "movie_test = np.zeros((num_features_test, num_movie_train))\n",
        "other_ratings_test = np.zeros((num_features_test, num_movie_train))\n",
        "time_test = (test_data[\"timestamp\"]-874724710) /18552992\n",
        "last_movie_test = np.zeros((num_features_test, num_movie_train))\n",
        "\n",
        "\n",
        "time_train = time_train.values.reshape(-1, 1)\n",
        "time_test = time_test.values.reshape(-1, 1)\n",
        "\n",
        "for i in range(num_features_test):\n",
        "  user_test[i, test_data.iloc[i]['user_id'] - 1] = 1\n",
        "  movie_test[i, test_data.iloc[i]['movie_id'] - 1] = 1\n",
        "  other_ratings_test[i, :] = normalized_ratings_test[test_data.iloc[i]['user_id'] - 1, :]\n",
        "  if i > 0:\n",
        "    if test_data.iloc[i - 1]['user_id'] == test_data.iloc[i]['user_id']:\n",
        "      last_movie_test[i, test_data.iloc[i - 1]['movie_id'] - 1] = 1\n",
        "\n",
        "\n",
        "train_features = np.concatenate([user_train, movie_train, other_ratings_train, time_train, last_movie_train], axis = 1)\n",
        "test_features = np.concatenate([user_test, movie_test, other_ratings_test, time_test, last_movie_test], axis = 1)"
      ],
      "metadata": {
        "id": "PwdQeeetBvO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"train_features.npy\", train_features)\n",
        "np.save(\"test_features.npy\", test_features)"
      ],
      "metadata": {
        "id": "mjYXFE_INpNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[\"timestamp\"].min())\n",
        "print(test_data[\"timestamp\"].min())\n",
        "print(train_data[\"timestamp\"].max())\n",
        "print(test_data[\"timestamp\"].max())\n",
        "print(893286638 - 874724727)\n",
        "print(893277702 - 874724710)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbbiun2ppXbU",
        "outputId": "65f2254c-41c2-4274-af89-426c29febb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "874724727\n",
            "874724710\n",
            "893286638\n",
            "893277702\n",
            "18561911\n",
            "18552992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_features.shape)\n",
        "print(test_features.shape)\n",
        "print(num_movie_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaIP4YWLOKtu",
        "outputId": "b6b8d0bb-7c16-48d0-874e-9d141920b22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80000, 5990)\n",
            "(20000, 5990)\n",
            "(80000,)\n",
            "1591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_user_train, num_movie_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn5SCg1pWEvS",
        "outputId": "13cd28ed-9e8d-4105-e2a2-6fce7a0d200c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "943 1682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_features = np.load(\"train_features.npy\")\n",
        "test_features = np.load(\"test_features.npy\")\n",
        "y_train = train_data[\"rating\"]\n",
        "y_test = test_data[\"rating\"]"
      ],
      "metadata": {
        "id": "t4xBLBC2Jylc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPwS5f7O_Qna",
        "outputId": "b1049afb-3d12-46cd-9c43-187e56c36fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.init as init\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class FM(nn.Module):\n",
        "  def __init__(self, train = train_features, y_train=y_train, y_test = y_test, num_user =943, num_movie = 1682, test=test_features, k=30, learning_rate=1e-3, epochs=100, device=device):\n",
        "    '''\n",
        "    FM\n",
        "    SVM처럼 general하게 적용 가능하지만 sparse한 데이터에서도 사용될 수 있도록 고안된 모델\n",
        "    degree = 2인 경우로\n",
        "\n",
        "    self.n: # of features\n",
        "    self.k: dimension\n",
        "    '''\n",
        "    super(FM, self).__init__()\n",
        "\n",
        "    self.train_features = train # (80000, 5990)\n",
        "    self.test_features = test   # (20000, 5990)\n",
        "    self.y_train = y_train # (80000,)\n",
        "    self.y_test = y_test   # (20000,)\n",
        "\n",
        "    self.train_features = torch.tensor(train, dtype=torch.float32, device=device)\n",
        "    self.test_features = torch.tensor(test, dtype=torch.float32, device=device)\n",
        "    self.y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
        "    self.y_test = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
        "\n",
        "    self.num_user = num_user\n",
        "    self.num_movie = 1682\n",
        "\n",
        "    self.n = self.train_features.shape[1]\n",
        "    self.num_features_train = self.train_features.shape[0]\n",
        "    self.num_features_test = self.test_features.shape[0]\n",
        "\n",
        "\n",
        "    # dimension\n",
        "    self.k = k\n",
        "\n",
        "    # parameter w0, wi, vi,f\n",
        "    self.w0 = nn.Parameter(init.normal_(torch.randn(1)), requires_grad=True)  # 상수 (1,)\n",
        "    self.w = nn.Parameter(init.normal_(torch.randn(self.n)), requires_grad=True)  # (n,)\n",
        "    self.V = nn.Parameter(init.normal_(torch.randn(self.n, self.k)), requires_grad=True)  # (n, k)\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.epoch = epochs\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input : (1, 5990) # V : (5990, k)\n",
        "    #\n",
        "    interactions = 0.5 * (torch.sum((torch.matmul(x, self.V))) ** 2 - torch.sum(torch.matmul(x, self.V)**2))\n",
        "    #print(\"interactions\", interactions.shape)\n",
        "    #print(\"interactions\", interactions)\n",
        "    y_hat = self.w0 + torch.matmul(x, self.w) + interactions\n",
        "    #print(\"y_hat\", y_hat.shape)\n",
        "    print(\"y_hat\", y_hat)\n",
        "    return y_hat\n",
        "\n",
        "  def loss(self, train = True):\n",
        "    loss = 0\n",
        "    if train:\n",
        "      for i in range(self.num_features_train):\n",
        "        y_hat = self.forward(self.train_features[i])\n",
        "        #loss += self.criterion(y_hat, self.y_train[i].unsqueeze(0))\n",
        "        loss += (y_hat - self.y_train[i].item()) ** 2\n",
        "    else:\n",
        "      for i in range(self.num_features_test):\n",
        "        y_hat = self.forward(self.test_features[i])\n",
        "        #loss += self.criterion(y_hat, self.y_test[i])\n",
        "        loss +=  (y_hat - self.y_test[i].item()) ** 2\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def fit(self):\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    self.optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "    for epoch in range(self.epoch):\n",
        "      #print(epoch)\n",
        "      self.train()\n",
        "      train_loss = self.loss()\n",
        "      self.optimizer.zero_grad()\n",
        "      train_loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      rmse_loss = torch.sqrt(train_loss / self.num_features_train)\n",
        "      train_loss_list.append(rmse_loss)\n",
        "\n",
        "\n",
        "      with torch.no_grad():\n",
        "        test_loss = self.loss(train = False)\n",
        "        rmse_test_loss = torch.sqrt(test_loss / self.num_features_test)\n",
        "      #if epoch % 20 == 0:\n",
        "      print(f'Epoch [{epoch}/{self.epoch}], train rmse: {rmse_loss}, test_rmse: {rmse_test_loss}')\n",
        "    return train_loss_list, test_loss_list\n"
      ],
      "metadata": {
        "id": "rMWknOoG1u7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = FM(epochs = 10).to(device)\n",
        "\n",
        "# Train the model\n",
        "train_loss_list, test_loss_list = model.fit()\n",
        "\n",
        "plt.plot(train_loss_list.cpu().numpy())\n",
        "plt.plot(test_loss_list.cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GUepMojN8z2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "time stamp 그대로 넣었더니 range 이상해짐\n",
        "\n",
        "=> normalize를 했는데\n",
        "\n",
        "=> torch.empty하면 값이 너무 큰 값도 들어감 ㄷ ㄷ ㄷ ㄷ (randn로 바꿈)\n",
        "\n",
        " sequential이 아닌데 timestamp가 필요할지??\n",
        "\n",
        " +) DataLoader의 중요성...\n",
        " 메모리를 아끼려면 하나씩 학습시키는게 맞다고 생각했는데 어쨌거나 DataLoader를 통해서 batch 만큼만 메모리에 올리는게 더 효율적\n",
        "\n",
        " 위와같이 코드를 작성하면 메모리에 train data가 전부 올라가서 매우 비효율적임\n",
        "\n",
        "\n",
        "\n",
        " ## GPU 이슈,, 나중에 다시 학습시켜 보기!"
      ],
      "metadata": {
        "id": "0D7_BYE3riH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_features = torch.tensor(np.load(\"train_features.npy\"), dtype=torch.float32, device=device)\n",
        "test_features = torch.tensor(np.load(\"test_features.npy\"), dtype=torch.float32, device=device)\n",
        "y_train = torch.tensor(train_data[\"rating\"], dtype=torch.float32, device=device)\n",
        "y_test = torch.tensor(test_data[\"rating\"], dtype=torch.float32, device=device)\n",
        "\n",
        "batch_size=64\n",
        "train_dataset = TensorDataset(train_features, y_train)\n",
        "test_dataset = TensorDataset(test_features, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xiMVv_BZ3RPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn.init as init\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class FM(nn.Module):\n",
        "  def __init__(self, train = train_features, y_train=y_train, y_test = y_test, num_user =943, num_movie = 1682, test=test_features, k=30, learning_rate=1e-3, epochs=100, constrain = False, device=device):\n",
        "    '''\n",
        "    FM\n",
        "    SVM처럼 general하게 적용 가능하지만 sparse한 데이터에서도 사용될 수 있도록 고안된 모델\n",
        "    degree = 2인 경우로\n",
        "\n",
        "    self.n: # of features\n",
        "    self.k: dimension\n",
        "    '''\n",
        "    super(FM, self).__init__()\n",
        "\n",
        "    self.train_features = train # (80000, 5990)\n",
        "    self.test_features = test   # (20000, 5990)\n",
        "    self.y_train = y_train # (80000,)\n",
        "    self.y_test = y_test   # (20000,)\n",
        "\n",
        "    self.train_features = torch.tensor(train, dtype=torch.float32, device=device)\n",
        "    self.test_features = torch.tensor(test, dtype=torch.float32, device=device)\n",
        "    self.y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
        "    self.y_test = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
        "\n",
        "    self.num_user = num_user\n",
        "    self.num_movie = 1682\n",
        "\n",
        "    self.n = self.train_features.shape[1]\n",
        "    self.num_features_train = self.train_features.shape[0]\n",
        "    self.num_features_test = self.test_features.shape[0]\n",
        "\n",
        "\n",
        "    # dimension\n",
        "    self.k = k\n",
        "\n",
        "    # parameter w0, wi, vi,f\n",
        "    self.w0 = nn.Parameter(init.normal_(torch.empty(1)), requires_grad=True)  # 상수 (1,)\n",
        "    self.w = nn.Parameter(init.normal_(torch.empty(self.n)), requires_grad=True)  # (n,)\n",
        "    self.V = nn.Parameter(init.normal_(torch.empty(self.n, self.k)), requires_grad=True)  # (n, k)\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.epoch = epochs\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input : (batch_size, 5990) # V : (5990, k)\n",
        "    #\n",
        "    interactions = interactions = 0.5 * (torch.sum((torch.matmul(x, self.V))) ** 2 - torch.sum(torch.matmul(x, self.V)**2))\n",
        "    y_hat = self.w0 + torch.matmul(x, self.w) + interactions\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "  def loss(self, train = True, x =None, y= None):\n",
        "    loss = 0\n",
        "    if train:\n",
        "      for i in range(len(x)):\n",
        "        y_hat = self.forward(x[i])\n",
        "        loss += self.criterion(y_hat, y[i])\n",
        "    else:\n",
        "      for i in range(self.num_features_test):\n",
        "        y_hat = self.forward(self.test_features[i])\n",
        "        loss += self.criterion(y_hat, self.y_test[i])\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def fit(self):\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    self.optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "    for epoch in range(self.epoch):\n",
        "      self.train()\n",
        "      for batch_x, batch_y in train_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        train_loss = self.loss( x=batch_x, y=batch_y)\n",
        "        rmse_loss = torch.sqrt(train_loss / len(train_loader.dataset))\n",
        "        train_loss_list.append(rmse_loss)\n",
        "        self.optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        test_loss = self.loss(train = False)\n",
        "        rmse_test_loss = torch.sqrt(test_loss / self.num_features_test)\n",
        "      if epoch % 20 == 0:\n",
        "        print(f'Epoch [{epoch}/{self.epoch}], train rmse: {rmse_loss}, test_rmse: {rmse_test_loss}')\n",
        "    return train_loss_list, test_loss_list\n"
      ],
      "metadata": {
        "id": "Aw6bin4OyGEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = FM(train_loader, test_loader).to(device)\n",
        "\n",
        "# Train the model\n",
        "train_loss_list, test_loss_list = model.fit()\n",
        "\n",
        "plt.plot(train_loss_list.cpu().numpy())\n",
        "plt.plot(test_loss_list.cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "lkiopR_-zXph",
        "outputId": "bbb1a172-a3ef-4065-d5cd-990d2852c76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-97ff16d1aea8>:69: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)\n",
            "  y_hat = self.w0 + torch.matmul(x, self.w.T) + interactions\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/100], train rmse: 0.06672803312540054, test_rmse: 2.456717014312744\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f6509aaf59e4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-97ff16d1aea8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}